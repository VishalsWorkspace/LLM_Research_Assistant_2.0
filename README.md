# **🔬 LLM Model Evaluation 2.0: Offline RAG with Flask & React**

This project showcases a robust, 100% offline, ChatGPT-style question-answering application for any PDF, built to demonstrate LLM performance metrics in a real-world Retrieval-Augmented Generation (RAG) setup. Version 2.0 transitions from Streamlit to a Flask API backend and a React.js frontend, providing a more stable and scalable architecture for model evaluation.

## **🚀 What's New in 2.0?**

* **Architectural Shift:** Replaces Streamlit with a dedicated **Flask API backend** and a **React.js single-page application (SPA) frontend**.  
* **Enhanced Stability:** Provides a more traditional web application structure for better performance and debugging.  
* **Offline First:** Still maintains 100% offline functionality, crucial for secure and private research.  
* **Foundation for Scalability:** Lays the groundwork for future online and mobile deployments (Version 3.0).

## **🔍 Core Functionality**

* **PDF Ingestion:** Uploads any PDF (data/sample.pdf) for processing.  
* **Question Answering:** Ask any question about the loaded PDF content.  
* **Local LLM:** Uses **Mistral 7B** running locally via **Ollama** for inference.  
* **Vector Search:** Embeds content with sentence-transformers and finds relevant context using **FAISS vector search**.  
* **Conversational AI:** Responds in full paragraphs, similar to ChatGPT, all executed offline.  
* **Real-time Performance Metrics:** 📊 Tracks and displays key LLM performance metrics directly in the UI.

## **📊 LLM Performance Metrics Tracked**

Each time a question is asked, the application provides insights into:

| Metric             | Description |
| :------------------|:--------------------------------------------- |
| ⏱️ Inference Time | Time taken for the LLM to generate a response |
| 🧠 LLM CPU Usage  | CPU % utilized by the Ollama process |
| 💾 LLM RAM Usage  | Resident Set Size (RSS) memory used by the LLM in MB |

## **📦 Requirements**

* Python 3.9 or above  
* pip install \-r requirements.txt  
* [Install Ollama](https://ollama.com/download) and run:  
  ollama run mistral

  *(First-time download may take a few GB. Once installed, it runs fully offline.)*  
* Node.js (for npm and serve to run the frontend)

## **📄 Sample PDF Included**

We’ve included the official LLM Technical Report (GPT-4) as a sample PDF:

* 📘 **GPT-4 Technical Report (arXiv)**  
* 📁 Located in: data/sample.pdf

🤖 **Try Prompts Like:**

* “What are the key differences between GPT-3.5 and GPT-4?”  
* “Mention benchmark scores”  
* “Limitations of GPT-4?”  
* “Summarize the performance of GPT-4 on academic tasks”  
* “What tests were used to evaluate GPT-4?”  
* “Does GPT-4 outperform humans?”

## **🖥️ Run Locally (Offline, No API Keys)**

Follow these steps to get Model Evaluation 2.0 running on your machine:

1. **Clone the repository:**  
   git clone https://github.com/YOUR\_USERNAME/llm\_research\_assistant.git  
   cd llm\_research\_assistant

2. **Create Python Virtual Environment & Install Dependencies:**  
   python \-m venv venv  
   .\\venv\\Scripts\\activate   \# On Windows PowerShell  
   \# Or: source venv/bin/activate \# On macOS/Linux/Git Bash  
   pip install \-r requirements.txt

3. Ensure Ollama is Running:  
   Open a NEW terminal window (do not activate venv here) and start the Ollama server:  
   ollama run mistral

   Keep this terminal open.  
4. Launch the Flask Backend API:  
   In your first terminal window (where venv is activated), run the Flask application:  
   python backend.py

   You should see output indicating the Flask app is running on http://127.0.0.1:5000. Keep this terminal open.  
5. Serve the React Frontend:  
   Open another NEW terminal window (do not activate venv here).  
   * **Install serve (if you don't have it):**  
     npm install \-g serve

   * **Navigate to your project directory:**  
     cd "D:\\B\\projects\\LLM\_research\_assistant" \# Or your actual project path

   * **Start the frontend server:**  
     serve .

     This will typically serve your React app on http://localhost:3000. Keep this terminal open.  
6. Access the Application:  
   Open your web browser and go to: http://localhost:3000

## **📁 Folder Structure**

llm\_research\_assistant/  
├── backend.py            ← Flask API handling PDF ingestion & LLM queries  
├── index.html            ← Main web application entry point  
├── index.js              ← React frontend application (contains App component logic)  
├── ingest.py             ← (Backend utility \- logic integrated into backend.py, can be removed if desired)  
├── requirements.txt      ← Python dependencies  
├── README.md  
├── .gitignore  
├── data/  
│   └── sample.pdf        ← GPT-4 Technical Report (sample PDF)  
├── vectorstore/          ← FAISS DB (auto-generated by backend.py)  
├── venv/                 ← Python Virtual Environment

## **🔐 Why Offline? (Key Benefits for Model Evaluation)**

* ✅ **No API Keys Needed:** 100% local operation means zero reliance on external APIs.  
* ✅ **True Offline Functionality:** Works completely without internet access once setup is complete.  
* ✅ **Enhanced Privacy & Security:** Your documents and data remain entirely on your local machine.  
* ✅ **Cost-Effective:** No cloud inference costs for LLM interactions.  
* ✅ **Ideal for Sensitive Data:** Perfect for evaluating models with confidential research, reports, or legal files.

## **🛣️ Future Roadmap: Version 3.0 (Online & Mobile)**

This **Model Evaluation 2.0** project serves as a robust foundation. Our future plans for **Version 3.0** include transforming this into a full-fledged, publicly accessible online and mobile application:

* **Cloud Deployment:** Hosting the Flask backend (with Ollama/LLM) on cloud infrastructure (e.g., AWS, GCP, Azure) for internet accessibility.  
* **Scalable Frontend Hosting:** Deploying the React frontend to a global static hosting service.  
* **Mobile Applications:** Developing native-like Android and iOS applications by wrapping the existing React web app (e.g., using Capacitor) or building dedicated mobile UIs (e.g., with React Native).  
* **User Authentication & Data Persistence:** Implementing user accounts and secure storage for PDFs and chat history (e.g., using Firebase Firestore).

## **👨‍💻 Author**

by Vishal

Star the repo ⭐ if you found it helpful — open to contributors\!