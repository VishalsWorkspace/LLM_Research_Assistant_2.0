# **ğŸ”¬ LLM Model Evaluation 2.0: Offline RAG with Flask & React**

This project showcases a robust, 100% offline, ChatGPT-style question-answering application for any PDF, built to demonstrate LLM performance metrics in a real-world Retrieval-Augmented Generation (RAG) setup. Version 2.0 transitions from Streamlit to a Flask API backend and a React.js frontend, providing a more stable and scalable architecture for model evaluation.

## **ğŸš€ What's New in 2.0?**

* **Architectural Shift:** Replaces Streamlit with a dedicated **Flask API backend** and a **React.js single-page application (SPA) frontend**.  
* **Enhanced Stability:** Provides a more traditional web application structure for better performance and debugging.  
* **Offline First:** Still maintains 100% offline functionality, crucial for secure and private research.  
* **Foundation for Scalability:** Lays the groundwork for future online and mobile deployments (Version 3.0).

## **ğŸ” Core Functionality**

* **PDF Ingestion:** Uploads any PDF (data/sample.pdf) for processing.  
* **Question Answering:** Ask any question about the loaded PDF content.  
* **Local LLM:** Uses **Mistral 7B** running locally via **Ollama** for inference.  
* **Vector Search:** Embeds content with sentence-transformers and finds relevant context using **FAISS vector search**.  
* **Conversational AI:** Responds in full paragraphs, similar to ChatGPT, all executed offline.  
* **Real-time Performance Metrics:** ğŸ“Š Tracks and displays key LLM performance metrics directly in the UI.

## **ğŸ“Š LLM Performance Metrics Tracked**

Each time a question is asked, the application provides insights into:

| Metric             | Description |
| :------------------|:--------------------------------------------- |
| â±ï¸ Inference Time | Time taken for the LLM to generate a response |
| ğŸ§  LLM CPU Usage  | CPU % utilized by the Ollama process |
| ğŸ’¾ LLM RAM Usage  | Resident Set Size (RSS) memory used by the LLM in MB |

## **ğŸ“¦ Requirements**

* Python 3.9 or above  
* pip install \-r requirements.txt  
* [Install Ollama](https://ollama.com/download) and run:  
  ollama run mistral

  *(First-time download may take a few GB. Once installed, it runs fully offline.)*  
* Node.js (for npm and serve to run the frontend)

## **ğŸ“„ Sample PDF Included**

Weâ€™ve included the official LLM Technical Report (GPT-4) as a sample PDF:

* ğŸ“˜ **GPT-4 Technical Report (arXiv)**  
* ğŸ“ Located in: data/sample.pdf

ğŸ¤– **Try Prompts Like:**

* â€œWhat are the key differences between GPT-3.5 and GPT-4?â€  
* â€œMention benchmark scoresâ€  
* â€œLimitations of GPT-4?â€  
* â€œSummarize the performance of GPT-4 on academic tasksâ€  
* â€œWhat tests were used to evaluate GPT-4?â€  
* â€œDoes GPT-4 outperform humans?â€

## **ğŸ–¥ï¸ Run Locally (Offline, No API Keys)**

Follow these steps to get Model Evaluation 2.0 running on your machine:

1. **Clone the repository:**  
   git clone https://github.com/YOUR\_USERNAME/llm\_research\_assistant.git  
   cd llm\_research\_assistant

2. **Create Python Virtual Environment & Install Dependencies:**  
   python \-m venv venv  
   .\\venv\\Scripts\\activate   \# On Windows PowerShell  
   \# Or: source venv/bin/activate \# On macOS/Linux/Git Bash  
   pip install \-r requirements.txt

3. Ensure Ollama is Running:  
   Open a NEW terminal window (do not activate venv here) and start the Ollama server:  
   ollama run mistral

   Keep this terminal open.  
4. Launch the Flask Backend API:  
   In your first terminal window (where venv is activated), run the Flask application:  
   python backend.py

   You should see output indicating the Flask app is running on http://127.0.0.1:5000. Keep this terminal open.  
5. Serve the React Frontend:  
   Open another NEW terminal window (do not activate venv here).  
   * **Install serve (if you don't have it):**  
     npm install \-g serve

   * **Navigate to your project directory:**  
     cd "D:\\B\\projects\\LLM\_research\_assistant" \# Or your actual project path

   * **Start the frontend server:**  
     serve .

     This will typically serve your React app on http://localhost:3000. Keep this terminal open.  
6. Access the Application:  
   Open your web browser and go to: http://localhost:3000

## **ğŸ“ Folder Structure**

llm\_research\_assistant/  
â”œâ”€â”€ backend.py            â† Flask API handling PDF ingestion & LLM queries  
â”œâ”€â”€ index.html            â† Main web application entry point  
â”œâ”€â”€ index.js              â† React frontend application (contains App component logic)  
â”œâ”€â”€ ingest.py             â† (Backend utility \- logic integrated into backend.py, can be removed if desired)  
â”œâ”€â”€ requirements.txt      â† Python dependencies  
â”œâ”€â”€ README.md  
â”œâ”€â”€ .gitignore  
â”œâ”€â”€ data/  
â”‚   â””â”€â”€ sample.pdf        â† GPT-4 Technical Report (sample PDF)  
â”œâ”€â”€ vectorstore/          â† FAISS DB (auto-generated by backend.py)  
â”œâ”€â”€ venv/                 â† Python Virtual Environment

## **ğŸ” Why Offline? (Key Benefits for Model Evaluation)**

* âœ… **No API Keys Needed:** 100% local operation means zero reliance on external APIs.  
* âœ… **True Offline Functionality:** Works completely without internet access once setup is complete.  
* âœ… **Enhanced Privacy & Security:** Your documents and data remain entirely on your local machine.  
* âœ… **Cost-Effective:** No cloud inference costs for LLM interactions.  
* âœ… **Ideal for Sensitive Data:** Perfect for evaluating models with confidential research, reports, or legal files.

## **ğŸ›£ï¸ Future Roadmap: Version 3.0 (Online & Mobile)**

This **Model Evaluation 2.0** project serves as a robust foundation. Our future plans for **Version 3.0** include transforming this into a full-fledged, publicly accessible online and mobile application:

* **Cloud Deployment:** Hosting the Flask backend (with Ollama/LLM) on cloud infrastructure (e.g., AWS, GCP, Azure) for internet accessibility.  
* **Scalable Frontend Hosting:** Deploying the React frontend to a global static hosting service.  
* **Mobile Applications:** Developing native-like Android and iOS applications by wrapping the existing React web app (e.g., using Capacitor) or building dedicated mobile UIs (e.g., with React Native).  
* **User Authentication & Data Persistence:** Implementing user accounts and secure storage for PDFs and chat history (e.g., using Firebase Firestore).

## **ğŸ‘¨â€ğŸ’» Author**

by Vishal

Star the repo â­ if you found it helpful â€” open to contributors\!