 ðŸ“‹ Table of Contents

  * [Introduction](#introduction)
  * [Features](#features)
  * [Getting Started](#getting-started)
      * [Prerequisites](#prerequisites)
      * [Installation](#installation)
      * [Running the Application](#running-the-application)
  * [ðŸ“‚ Project Structure](#-project-structure)
  * [Usage](#usage)
  * [Configuration](#configuration)
  * [ðŸ©º Troubleshooting](#-troubleshooting)
  * [Contributing](#contributing)
  * [License](#license)

-----

## Introduction

**LLM-Research\_Assistant 2.0** is an intelligent assistant designed to streamline your research workflow by allowing you to interact directly with your PDF documents using **Large Language Models (LLMs)**. This tool enables you to upload PDF files, embed their content for efficient retrieval, and then ask questions about the documents, receiving concise and relevant answers directly from the LLM.

This project runs 100% locally, using Ollama to ensure data privacy and reduce reliance on external APIs.

## âœ¨ Features

  * **PDF Upload & Processing:** Easily upload PDF documents for analysis.
  * **Vector Database Integration:** Uses FAISS to efficiently embed PDF content into a vector store for fast, accurate retrieval.
  * **LLM-Powered Q\&A:** Ask natural language questions and get answers generated by a local LLM (e.g., Mistral, Llama 2).
  * **Simple Web Interface:** A clean HTML/JS/CSS frontend to upload files and manage your chat.
  * **Local-First:** Designed to work with local LLM inference servers like Ollama.

-----

## ðŸš€ Getting Started

Follow these steps to set up and run the project on your local machine.

### Prerequisites

Before you begin, ensure you have the following installed:

  * **Python 3.9+**
  * **pip** (Python package installer)
  * **Git** (for cloning the repository)
  * **A Local LLM Inference Server:**
      * **Ollama (Recommended):** Download and install [Ollama](https://ollama.com/download).
      * After installation, pull a model. Your logs show you are using **Mistral**, so run:
        ```bash
        ollama pull mistral
        ```
        *(Alternatively, `ollama pull llama2` also works if configured in `backend.py`).*

### Installation

1.  **Clone the Repository:**

    ```bash
    git clone [https://github.com/your-username/LLM-Research_assistant-2.0.git](https://github.com/your-username/LLM-Research_assistant-2.0.git)
    cd LLM-Research_assistant-2.0
    ```

    (Remember to replace `your-username` with your actual GitHub username.)

2.  **Create a Virtual Environment:**

    ```bash
    python -m venv venv
    ```

3.  **Activate the Virtual Environment:**

      * **Windows:**
        ```bash
        .\venv\Scripts\activate
        ```
      * **macOS/Linux:**
        ```bash
        source venv/bin/activate
        ```

4.  **Install Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

### Running the Application

This project requires **two** servers to be running on your computer.

#### 1\. (Optional) Run the LLM Server

Your Ollama server might already be running as a background service. You can check by opening a new terminal and running `ollama serve`.

  * If you get `Error: listen tcp 127.0.0.1:11434: bind...`, **it's already running\!** You can close this terminal and proceed.
  * If it starts the server, just **leave this terminal running** in the background.

#### 2\. Run Your Backend Server

In your main project terminal (where you activated `venv`):

1.  **Start the Flask application:**

    ```bash
    python backend.py
    ```

    *(Note: The main application file is `backend.py`, not `main.py` or `app.py`).*

2.  You should see output indicating the server is running on `http://127.0.0.1:5000`.

#### 3\. Access the Application

  * Open your web browser and go to: **`http://127.0.0.1:5000`**
  * You should now see your `index.html` page, and you can start uploading PDFs.

-----

## ðŸ“‚ Project Structure

This project uses a "flat" structure, meaning most files are in the root directory for simplicity.

```
LLM-Research_assistant-2.0/
â”œâ”€â”€ venv/                # Python virtual environment
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vectorstore/     # Stores the FAISS vector index after PDF upload
â”œâ”€â”€ .gitignore           # Files for Git to ignore
â”œâ”€â”€ app.js               # Frontend JavaScript for handling chat/uploads
â”œâ”€â”€ backend.py           # The main Python Flask backend server
â”œâ”€â”€ index.html           # The single-page web interface (frontend)
â”œâ”€â”€ index.js             # (Additional frontend JavaScript, if any)
â”œâ”€â”€ README.md            # This documentation file
â””â”€â”€ requirements.txt     # List of Python dependencies
```

-----

## Usage

### Uploading PDFs

1.  Access the application in your web browser (`http://127.0.0.1:5000`).
2.  Use the "Upload PDF" button to select one or more PDF files.
3.  The system will process them, create a vector store, and save it to the `data/vectorstore` directory. You should receive a confirmation message.

### Asking Questions

1.  Once a PDF is successfully processed, the chat interface will be ready.
2.  Type your question related to the uploaded documents into the input field.
3.  Submit your question. The backend will query the vector store, pass the context to Ollama, and stream the answer back to the chat.

-----

## Configuration

  * **LLM Model:** The LLM model is configured inside `backend.py`. You can change `Ollama(model="mistral")` to any other model you have downloaded (e.g., `"llama2"`).
  * **Ollama Endpoint:** The application connects to `http://localhost:11434` by default. If your Ollama server is on a different port, you must update this in `backend.py`.

-----

## ðŸ©º Troubleshooting

  * **Error:** `Error: listen tcp 127.0.0.1:11434: bind: Only one usage...`

      * **Cause:** You tried to run `ollama serve`, but it's *already running* in the background.
      * **Solution:** This is not an error. Your Ollama server is ready. You can close this terminal and run `python backend.py` in your other terminal.

  * **Error:** In browser, you see `Not Found`. In the terminal, you see `GET / HTTP/1.1" 404 -`.

      * **Cause:** Your `backend.py` (Flask server) isn't correctly configured to send your `index.html` file to the browser.
      * **Solution:** Open `backend.py` and ensure you have a route for `/` that serves the `index.html` file. You may also need routes for `app.js` and `index.js`.

  * **Warning:** `FAISS vectorstore not found or error loading...`

      * **Cause:** This is a normal warning. It means you haven't uploaded any PDFs yet, so the `data/vectorstore/index.faiss` file doesn't exist.
      * **Solution:** Simply upload a PDF through the web interface. The server will create this file, and the warning will go away on the next restart.

  * **Error:** `ConnectionRefusedError` or `Max retries exceeded` in the `backend.py` log.

      * **Cause:** Your `backend.py` server cannot connect to the Ollama LLM server.
      * **Solution:** Ensure your Ollama server is running. Open a *new* terminal and run `ollama serve`, then leave it running.

-----

## Contributing

We welcome contributions\! Please fork the repository, create a new branch, make your changes, and open a Pull Request.

1.  Fork the repository.
2.  Create a feature branch: `git checkout -b feature/your-feature-name`
3.  Commit your changes: `git commit -m 'Add new feature'`
4.  Push to the branch: `git push origin feature/your-feature-name`
5.  Open a Pull Request.

-----

## License

This project is licensed under the **MIT License** - see the `LICENSE` file for details.
